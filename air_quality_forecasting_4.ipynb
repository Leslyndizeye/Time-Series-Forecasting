{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiP6+vsNIhOq1lGDEwLqoQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leslyndizeye/Time-Series-Forecasting/blob/main/air_quality_forecasting_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPORT LIBRARIES"
      ],
      "metadata": {
        "id": "r9JLZV3t07O_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kwZK5kk80tLZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# TensorFlow imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (LSTM, Dense, Dropout, Bidirectional, GRU,\n",
        "                                   Conv1D, MaxPooling1D, Flatten, Input,\n",
        "                                   Concatenate, Attention, MultiHeadAttention,\n",
        "                                   LayerNormalization, GlobalAveragePooling1D)\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Configuration\n",
        "plt.style.use('default')\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA LOADING & ENHANCED PREPROCESSING"
      ],
      "metadata": {
        "id": "KvDuauTK1vTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load datasets\n",
        "train = pd.read_csv('/content/drive/My Drive/air_quality/data/train.csv')\n",
        "test = pd.read_csv('/content/drive/My Drive/air_quality/data/test.csv')\n",
        "\n",
        "print(\" Data Loading Complete:\")\n",
        "print(f\"Training Data Shape: {train.shape}\")\n",
        "print(f\"Test Data Shape: {test.shape}\")\n",
        "\n",
        "# Enhanced feature engineering with more advanced features\n",
        "def create_advanced_features_v2(df, is_training=True):\n",
        "    \"\"\"Enhanced feature engineering with more sophisticated features\"\"\"\n",
        "    df = df.copy()\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "\n",
        "    # Extended time features\n",
        "    df['hour'] = df['datetime'].dt.hour\n",
        "    df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "    df['month'] = df['datetime'].dt.month\n",
        "    df['quarter'] = df['datetime'].dt.quarter\n",
        "    df['day_of_year'] = df['datetime'].dt.dayofyear\n",
        "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
        "    df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)\n",
        "    df['is_rush_hour'] = ((df['hour'] >= 7) & (df['hour'] <= 9)) | ((df['hour'] >= 16) & (df['hour'] <= 18))\n",
        "    df['is_rush_hour'] = df['is_rush_hour'].astype(int)\n",
        "\n",
        "    # Advanced cyclical encoding\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "\n",
        "    # Advanced weather interactions\n",
        "    df['temp_dew_diff'] = df['TEMP'] - df['DEWP']\n",
        "    df['wind_pressure'] = df['Iws'] * df['PRES']\n",
        "    df['temp_humidity_index'] = df['TEMP'] * df['DEWP'] / 100\n",
        "    df['wind_temp_ratio'] = df['Iws'] / (df['TEMP'] + 1e-6)\n",
        "    df['pressure_normalized'] = (df['PRES'] - df['PRES'].mean()) / df['PRES'].std()\n",
        "\n",
        "    # Polynomial features\n",
        "    df['temp_squared'] = df['TEMP'] ** 2\n",
        "    df['dew_squared'] = df['DEWP'] ** 2\n",
        "    df['wind_squared'] = df['Iws'] ** 2\n",
        "\n",
        "    # Only create PM2.5 based features for training data\n",
        "    if is_training and 'pm2.5' in df.columns:\n",
        "        # Advanced rolling statistics\n",
        "        windows = [3, 6, 12, 24, 48]\n",
        "        for window in windows:\n",
        "            df[f'pm2.5_roll_mean_{window}'] = df['pm2.5'].rolling(window=window, min_periods=1).mean()\n",
        "            df[f'pm2.5_roll_std_{window}'] = df['pm2.5'].rolling(window=window, min_periods=1).std()\n",
        "            df[f'pm2.5_roll_min_{window}'] = df['pm2.5'].rolling(window=window, min_periods=1).min()\n",
        "            df[f'pm2.5_roll_max_{window}'] = df['pm2.5'].rolling(window=window, min_periods=1).max()\n",
        "            df[f'pm2.5_roll_range_{window}'] = df[f'pm2.5_roll_max_{window}'] - df[f'pm2.5_roll_min_{window}']\n",
        "\n",
        "        # Extended lag features\n",
        "        lags = [1, 2, 3, 6, 12, 24, 48]\n",
        "        for lag in lags:\n",
        "            df[f'pm2.5_lag_{lag}'] = df['pm2.5'].shift(lag)\n",
        "\n",
        "        # Trend features\n",
        "        df['pm2.5_trend_1h'] = df['pm2.5'] - df['pm2.5'].shift(1)\n",
        "        df['pm2.5_trend_3h'] = df['pm2.5'] - df['pm2.5'].shift(3)\n",
        "        df['pm2.5_trend_6h'] = df['pm2.5'] - df['pm2.5'].shift(6)\n",
        "        df['pm2.5_trend_12h'] = df['pm2.5'] - df['pm2.5'].shift(12)\n",
        "\n",
        "        # Momentum features\n",
        "        df['pm2.5_momentum_3h'] = df['pm2.5_trend_1h'].rolling(window=3).mean()\n",
        "        df['pm2.5_momentum_6h'] = df['pm2.5_trend_1h'].rolling(window=6).mean()\n",
        "\n",
        "    # Weather lag features for both train and test\n",
        "    weather_cols = ['TEMP', 'DEWP', 'PRES', 'Iws', 'Is', 'Ir']\n",
        "    for col in weather_cols:\n",
        "        if col in df.columns:\n",
        "            for lag in [1, 3, 6, 12]:\n",
        "                df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
        "\n",
        "    # Interaction features between weather variables\n",
        "    df['temp_dew_pressure'] = df['TEMP'] * df['DEWP'] * df['PRES']\n",
        "    df['wind_is_interaction'] = df['Iws'] * df['Is']\n",
        "    df['wind_ir_interaction'] = df['Iws'] * df['Ir']\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply enhanced feature engineering\n",
        "train_filled = train.ffill().bfill().fillna(train.mean(numeric_only=True))\n",
        "test_filled = test.ffill().bfill().fillna(test.mean(numeric_only=True))\n",
        "\n",
        "print(\" Creating advanced features v2...\")\n",
        "train_enhanced = create_advanced_features_v2(train_filled, is_training=True)\n",
        "test_enhanced = create_advanced_features_v2(test_filled, is_training=False)\n",
        "\n",
        "# Handle NaN values\n",
        "train_enhanced = train_enhanced.ffill().bfill().fillna(0)\n",
        "test_enhanced = test_enhanced.ffill().bfill().fillna(0)\n",
        "\n",
        "print(f\" Enhanced features - Train: {train_enhanced.shape}, Test: {test_enhanced.shape}\")\n",
        "\n",
        "# Prepare features and target\n",
        "X_train = train_enhanced.drop(['pm2.5', 'No', 'datetime'], axis=1, errors='ignore')\n",
        "y_train = train_enhanced['pm2.5']\n",
        "X_test = test_enhanced.drop(['No', 'datetime'], axis=1, errors='ignore')\n",
        "\n",
        "# Ensure both have the same columns\n",
        "common_cols = list(set(X_train.columns) & set(X_test.columns))\n",
        "X_train = X_train[common_cols]\n",
        "X_test = X_test[common_cols]\n",
        "\n",
        "print(f\" Common features: {len(common_cols)}\")\n",
        "\n",
        "# Feature selection with more features\n",
        "selector = SelectKBest(score_func=f_regression, k=min(40, len(common_cols)))\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "selected_mask = selector.get_support()\n",
        "selected_features = X_train.columns[selected_mask]\n",
        "\n",
        "X_train = X_train[selected_features]\n",
        "X_test = X_test[selected_features]\n",
        "\n",
        "print(f\" Selected {len(selected_features)} best features\")\n",
        "\n",
        "# Scale features using RobustScaler\n",
        "scaler = RobustScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQITTVZq1zbm",
        "outputId": "9488431e-76af-4932-ef1d-0979e4398bc6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            " Data Loading Complete:\n",
            "Training Data Shape: (30676, 12)\n",
            "Test Data Shape: (13148, 11)\n",
            " Creating advanced features v2...\n",
            " Enhanced features - Train: (30676, 99), Test: (13148, 60)\n",
            " Common features: 58\n",
            " Selected 40 best features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MODEL ARCHITECTURES"
      ],
      "metadata": {
        "id": "W5nZ6oLS2h21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequences for LSTM\n",
        "def create_sequences(X, y, time_steps=48):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        Xs.append(X[i:(i + time_steps)])\n",
        "        ys.append(y[i + time_steps])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "TIME_STEPS = 48  # Longer sequences for better temporal patterns\n",
        "X_seq, y_seq = create_sequences(X_train_scaled, y_train.values, TIME_STEPS)\n",
        "\n",
        "# Split data\n",
        "split_idx = int(0.8 * len(X_seq))\n",
        "X_train_seq, X_val_seq = X_seq[:split_idx], X_seq[split_idx:]\n",
        "y_train_seq, y_val_seq = y_seq[:split_idx], y_seq[split_idx:]\n",
        "\n",
        "print(f\" Sequential data - Train: {X_train_seq.shape}, Val: {X_val_seq.shape}\")\n",
        "\n",
        "# 1. CNN-LSTM Hybrid Model\n",
        "def create_cnn_lstm_model(input_shape):\n",
        "    \"\"\"CNN-LSTM hybrid architecture\"\"\"\n",
        "    model = Sequential([\n",
        "        # CNN layers for feature extraction\n",
        "        Conv1D(filters=64, kernel_size=3, activation='relu',\n",
        "               input_shape=input_shape, padding='same'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "\n",
        "        # LSTM layers for sequence processing\n",
        "        Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        Dropout(0.3),\n",
        "        Bidirectional(LSTM(32)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # Dense layers\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0005),\n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# 2. Transformer-inspired Architecture\n",
        "def create_transformer_model(input_shape):\n",
        "    \"\"\"Transformer-inspired architecture for time series\"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Positional encoding (simplified)\n",
        "    x = LayerNormalization()(inputs)\n",
        "\n",
        "    # Multi-head self-attention\n",
        "    attention_output = MultiHeadAttention(num_heads=4, key_dim=input_shape[1])(x, x)\n",
        "    x = tf.keras.layers.Add()([x, attention_output])\n",
        "    x = LayerNormalization()(x)\n",
        "\n",
        "    # Feed-forward network\n",
        "    ff_output = Dense(64, activation='relu')(x)\n",
        "    ff_output = Dense(input_shape[1])(ff_output)\n",
        "    x = tf.keras.layers.Add()([x, ff_output])\n",
        "    x = LayerNormalization()(x)\n",
        "\n",
        "    # Global pooling and output\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0005),\n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# 3. GRU with Attention\n",
        "def create_gru_attention_model(input_shape):\n",
        "    \"\"\"GRU with attention mechanism\"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # GRU layers\n",
        "    x = Bidirectional(GRU(64, return_sequences=True))(inputs)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Bidirectional(GRU(32, return_sequences=True))(x)\n",
        "\n",
        "    # Attention mechanism\n",
        "    attention = Attention()([x, x])\n",
        "    x = tf.keras.layers.Concatenate()([x, attention])\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "    # Output layers\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0005),\n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# 4. Enhanced Bidirectional LSTM (baseline for comparison)\n",
        "def create_enhanced_lstm_model(input_shape):\n",
        "    \"\"\"Enhanced LSTM architecture\"\"\"\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(128, return_sequences=True,\n",
        "                          kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
        "                          input_shape=input_shape)),\n",
        "        Dropout(0.3),\n",
        "        Bidirectional(LSTM(64, return_sequences=True,\n",
        "                          kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))),\n",
        "        Dropout(0.3),\n",
        "        Bidirectional(LSTM(32)),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0005),\n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# Create all models\n",
        "models = {\n",
        "    'CNN-LSTM': create_cnn_lstm_model((X_train_seq.shape[1], X_train_seq.shape[2])),\n",
        "    'Transformer': create_transformer_model((X_train_seq.shape[1], X_train_seq.shape[2])),\n",
        "    'GRU-Attention': create_gru_attention_model((X_train_seq.shape[1], X_train_seq.shape[2])),\n",
        "    'Enhanced-LSTM': create_enhanced_lstm_model((X_train_seq.shape[1], X_train_seq.shape[2]))\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNdMFmOZ2ofb",
        "outputId": "43b55acf-6b4f-459c-bb8e-89ee75acd5c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sequential data - Train: (24502, 48, 40), Val: (6126, 48, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " MODEL TRAINING & ENSEMBLE"
      ],
      "metadata": {
        "id": "td3fi4JT3Jdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_predictions = {}\n",
        "model_performance = {}\n",
        "trained_models = {}\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6),\n",
        "    ModelCheckpoint('/content/best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "]\n",
        "\n",
        "# Train each model\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n Training {name} model...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train_seq, y_train_seq,\n",
        "        validation_data=(X_val_seq, y_val_seq),\n",
        "        epochs=20,\n",
        "        batch_size=64,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    val_pred = model.predict(X_val_seq, verbose=0)\n",
        "    val_rmse = np.sqrt(mean_squared_error(y_val_seq, val_pred))\n",
        "    val_mae = mean_absolute_error(y_val_seq, val_pred)\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    model_performance[name] = {\n",
        "        'rmse': val_rmse,\n",
        "        'mae': val_mae,\n",
        "        'time': training_time\n",
        "    }\n",
        "\n",
        "    trained_models[name] = model\n",
        "    model_predictions[name] = val_pred\n",
        "\n",
        "    print(f\" {name} - RMSE: {val_rmse:.2f}, MAE: {val_mae:.2f}, Time: {training_time:.1f}s\")\n",
        "\n",
        "# Display performance comparison\n",
        "print(\"\\n MODEL PERFORMANCE COMPARISON:\")\n",
        "performance_df = pd.DataFrame(model_performance).T\n",
        "print(performance_df.sort_values('rmse'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWwnFwUP3LQC",
        "outputId": "82f875b6-b3af-4020-8615-3b1f381fa194"
      },
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Training CNN-LSTM model...\n",
            "Epoch 1/40\n",
            "\u001b[1m382/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 12422.4883 - mae: 78.9233"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 74ms/step - loss: 12406.5244 - mae: 78.8720 - val_loss: 8423.0898 - val_mae: 63.1609 - learning_rate: 5.0000e-04\n",
            "Epoch 2/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 4955.7271 - mae: 49.3647"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 74ms/step - loss: 4954.5083 - mae: 49.3573 - val_loss: 5856.2446 - val_mae: 51.6159 - learning_rate: 5.0000e-04\n",
            "Epoch 3/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 3913.7556 - mae: 42.3369"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 76ms/step - loss: 3913.1609 - mae: 42.3335 - val_loss: 5700.0288 - val_mae: 48.4007 - learning_rate: 5.0000e-04\n",
            "Epoch 4/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 3351.4199 - mae: 39.0266"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 73ms/step - loss: 3350.9895 - mae: 39.0243 - val_loss: 5516.6758 - val_mae: 47.9236 - learning_rate: 5.0000e-04\n",
            "Epoch 5/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 73ms/step - loss: 2935.0027 - mae: 36.4711 - val_loss: 5617.8784 - val_mae: 49.9121 - learning_rate: 5.0000e-04\n",
            "Epoch 6/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 73ms/step - loss: 2639.1487 - mae: 34.8478 - val_loss: 5630.6841 - val_mae: 49.3024 - learning_rate: 5.0000e-04\n",
            "Epoch 7/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 73ms/step - loss: 2446.8313 - mae: 33.4321 - val_loss: 5718.6216 - val_mae: 50.0917 - learning_rate: 5.0000e-04\n",
            "Epoch 8/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 73ms/step - loss: 2217.5967 - mae: 31.9911 - val_loss: 5602.4956 - val_mae: 49.3787 - learning_rate: 5.0000e-04\n",
            "Epoch 9/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 74ms/step - loss: 2012.6056 - mae: 30.6680 - val_loss: 5547.8994 - val_mae: 48.6311 - learning_rate: 5.0000e-04\n",
            "Epoch 10/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 1745.1106 - mae: 28.5462"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 70ms/step - loss: 1745.0286 - mae: 28.5455 - val_loss: 5413.1777 - val_mae: 48.2280 - learning_rate: 2.5000e-04\n",
            "Epoch 11/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 70ms/step - loss: 1643.4750 - mae: 27.8585 - val_loss: 5475.1699 - val_mae: 48.9841 - learning_rate: 2.5000e-04\n",
            "Epoch 12/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 75ms/step - loss: 1593.7812 - mae: 27.0891 - val_loss: 5669.4795 - val_mae: 49.7336 - learning_rate: 2.5000e-04\n",
            "Epoch 13/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 70ms/step - loss: 1498.4912 - mae: 26.4464 - val_loss: 5541.4834 - val_mae: 48.9130 - learning_rate: 2.5000e-04\n",
            "Epoch 14/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 68ms/step - loss: 1406.3370 - mae: 25.8179 - val_loss: 5577.5278 - val_mae: 49.1341 - learning_rate: 2.5000e-04\n",
            "Epoch 15/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 75ms/step - loss: 1369.8870 - mae: 25.4896 - val_loss: 5801.0015 - val_mae: 49.8294 - learning_rate: 2.5000e-04\n",
            "Epoch 16/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 69ms/step - loss: 1265.3225 - mae: 24.4338 - val_loss: 5668.7974 - val_mae: 50.4594 - learning_rate: 1.2500e-04\n",
            "Epoch 17/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 77ms/step - loss: 1223.7616 - mae: 24.1828 - val_loss: 5803.2666 - val_mae: 50.2506 - learning_rate: 1.2500e-04\n",
            "Epoch 18/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 70ms/step - loss: 1189.3171 - mae: 23.8281 - val_loss: 5738.5117 - val_mae: 49.9079 - learning_rate: 1.2500e-04\n",
            "Epoch 19/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 67ms/step - loss: 1156.6709 - mae: 23.4379 - val_loss: 5897.5659 - val_mae: 50.6339 - learning_rate: 1.2500e-04\n",
            "Epoch 20/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 69ms/step - loss: 1144.8472 - mae: 23.2931 - val_loss: 5784.0381 - val_mae: 49.9744 - learning_rate: 1.2500e-04\n",
            " CNN-LSTM - RMSE: 73.57, MAE: 48.23, Time: 716.2s\n",
            "\n",
            " Training Transformer model...\n",
            "Epoch 1/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 83ms/step - loss: 14501.8008 - mae: 84.6805 - val_loss: 11681.2002 - val_mae: 75.1018 - learning_rate: 5.0000e-04\n",
            "Epoch 2/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 77ms/step - loss: 6835.9854 - mae: 57.4505 - val_loss: 8762.9697 - val_mae: 64.1508 - learning_rate: 5.0000e-04\n",
            "Epoch 3/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 77ms/step - loss: 4941.7065 - mae: 48.3850 - val_loss: 8067.9619 - val_mae: 62.2280 - learning_rate: 5.0000e-04\n",
            "Epoch 4/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 77ms/step - loss: 4033.3130 - mae: 43.7500 - val_loss: 7607.1016 - val_mae: 60.8058 - learning_rate: 5.0000e-04\n",
            "Epoch 5/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 77ms/step - loss: 3542.4456 - mae: 41.1323 - val_loss: 7562.0430 - val_mae: 60.2024 - learning_rate: 5.0000e-04\n",
            "Epoch 6/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 77ms/step - loss: 3278.4429 - mae: 39.3906 - val_loss: 7480.6016 - val_mae: 59.4222 - learning_rate: 5.0000e-04\n",
            "Epoch 7/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 77ms/step - loss: 3102.7395 - mae: 38.1618 - val_loss: 7638.2539 - val_mae: 59.4875 - learning_rate: 5.0000e-04\n",
            "Epoch 8/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 76ms/step - loss: 2918.3438 - mae: 37.0704 - val_loss: 7680.1973 - val_mae: 59.6126 - learning_rate: 5.0000e-04\n",
            "Epoch 9/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 77ms/step - loss: 2745.2532 - mae: 35.9244 - val_loss: 7818.3662 - val_mae: 60.1005 - learning_rate: 5.0000e-04\n",
            "Epoch 10/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 77ms/step - loss: 2609.8850 - mae: 35.0889 - val_loss: 7811.0493 - val_mae: 59.9659 - learning_rate: 5.0000e-04\n",
            "Epoch 11/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 77ms/step - loss: 2485.0688 - mae: 34.1678 - val_loss: 7988.5713 - val_mae: 60.3813 - learning_rate: 5.0000e-04\n",
            "Epoch 12/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 83ms/step - loss: 2353.1174 - mae: 33.0320 - val_loss: 8142.6001 - val_mae: 60.3600 - learning_rate: 2.5000e-04\n",
            "Epoch 13/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 76ms/step - loss: 2282.9651 - mae: 32.6904 - val_loss: 8197.0986 - val_mae: 60.6333 - learning_rate: 2.5000e-04\n",
            "Epoch 14/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 76ms/step - loss: 2223.4146 - mae: 32.2115 - val_loss: 8431.2803 - val_mae: 61.1318 - learning_rate: 2.5000e-04\n",
            "Epoch 15/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 76ms/step - loss: 2183.0215 - mae: 31.8756 - val_loss: 8505.5752 - val_mae: 61.2528 - learning_rate: 2.5000e-04\n",
            "Epoch 16/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 76ms/step - loss: 2135.4009 - mae: 31.6315 - val_loss: 8692.4229 - val_mae: 61.9023 - learning_rate: 2.5000e-04\n",
            " Transformer - RMSE: 86.49, MAE: 59.42, Time: 606.3s\n",
            "\n",
            " Training GRU-Attention model...\n",
            "Epoch 1/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 163ms/step - loss: 12087.1738 - mae: 76.9900 - val_loss: 7210.8423 - val_mae: 56.6297 - learning_rate: 5.0000e-04\n",
            "Epoch 2/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 162ms/step - loss: 4769.7275 - mae: 47.9281 - val_loss: 6060.9487 - val_mae: 50.9808 - learning_rate: 5.0000e-04\n",
            "Epoch 3/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 159ms/step - loss: 4005.9673 - mae: 43.5042 - val_loss: 6031.8354 - val_mae: 51.9015 - learning_rate: 5.0000e-04\n",
            "Epoch 4/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 161ms/step - loss: 3670.3149 - mae: 41.4646 - val_loss: 5554.1450 - val_mae: 50.2243 - learning_rate: 5.0000e-04\n",
            "Epoch 5/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 158ms/step - loss: 3372.4736 - mae: 39.3225 - val_loss: 5707.2559 - val_mae: 50.8486 - learning_rate: 5.0000e-04\n",
            "Epoch 6/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 3288.7520 - mae: 38.7940"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 157ms/step - loss: 3288.5425 - mae: 38.7926 - val_loss: 5312.2847 - val_mae: 48.4452 - learning_rate: 5.0000e-04\n",
            "Epoch 7/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 155ms/step - loss: 3089.5129 - mae: 37.3938 - val_loss: 5675.5918 - val_mae: 48.6585 - learning_rate: 5.0000e-04\n",
            "Epoch 8/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 2974.2092 - mae: 36.5208"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 161ms/step - loss: 2974.0786 - mae: 36.5202 - val_loss: 5231.9746 - val_mae: 47.4516 - learning_rate: 5.0000e-04\n",
            "Epoch 9/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 155ms/step - loss: 2824.5845 - mae: 35.5273 - val_loss: 5337.0151 - val_mae: 47.3150 - learning_rate: 5.0000e-04\n",
            "Epoch 10/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 2804.5728 - mae: 35.3106"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 161ms/step - loss: 2804.4832 - mae: 35.3100 - val_loss: 5167.7896 - val_mae: 46.2713 - learning_rate: 5.0000e-04\n",
            "Epoch 11/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 2640.3362 - mae: 34.4620"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 161ms/step - loss: 2640.2329 - mae: 34.4613 - val_loss: 5164.2383 - val_mae: 46.4017 - learning_rate: 5.0000e-04\n",
            "Epoch 12/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 155ms/step - loss: 2537.9465 - mae: 33.7830 - val_loss: 5301.7139 - val_mae: 46.2964 - learning_rate: 5.0000e-04\n",
            "Epoch 13/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 155ms/step - loss: 2449.6648 - mae: 33.0812 - val_loss: 5211.3599 - val_mae: 45.4903 - learning_rate: 5.0000e-04\n",
            "Epoch 14/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 161ms/step - loss: 2301.2874 - mae: 32.1203 - val_loss: 5263.5850 - val_mae: 45.9777 - learning_rate: 5.0000e-04\n",
            "Epoch 15/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 156ms/step - loss: 2236.5735 - mae: 31.7276 - val_loss: 5172.9844 - val_mae: 45.5823 - learning_rate: 5.0000e-04\n",
            "Epoch 16/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 156ms/step - loss: 2135.9377 - mae: 30.9974 - val_loss: 5758.2666 - val_mae: 47.3691 - learning_rate: 5.0000e-04\n",
            "Epoch 17/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 159ms/step - loss: 2041.0892 - mae: 30.3260 - val_loss: 5164.8916 - val_mae: 45.7645 - learning_rate: 2.5000e-04\n",
            "Epoch 18/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 159ms/step - loss: 1969.9406 - mae: 29.7240 - val_loss: 5178.4062 - val_mae: 45.6514 - learning_rate: 2.5000e-04\n",
            "Epoch 19/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 154ms/step - loss: 1865.9962 - mae: 29.0870 - val_loss: 5339.1060 - val_mae: 46.2525 - learning_rate: 2.5000e-04\n",
            "Epoch 20/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 159ms/step - loss: 1905.5162 - mae: 29.2942 - val_loss: 5325.5308 - val_mae: 45.9722 - learning_rate: 2.5000e-04\n",
            "Epoch 21/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 153ms/step - loss: 1827.1578 - mae: 28.7225 - val_loss: 5321.8804 - val_mae: 46.0997 - learning_rate: 2.5000e-04\n",
            " GRU-Attention - RMSE: 71.86, MAE: 46.40, Time: 1506.0s\n",
            "\n",
            " Training Enhanced-LSTM model...\n",
            "Epoch 1/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 436ms/step - loss: 11262.1865 - mae: 73.3244 - val_loss: 6103.1309 - val_mae: 50.9386 - learning_rate: 5.0000e-04\n",
            "Epoch 2/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 409ms/step - loss: 3766.7788 - mae: 41.4011 - val_loss: 6116.4458 - val_mae: 51.2344 - learning_rate: 5.0000e-04\n",
            "Epoch 3/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 411ms/step - loss: 2934.4321 - mae: 36.4707 - val_loss: 5588.2715 - val_mae: 48.6762 - learning_rate: 5.0000e-04\n",
            "Epoch 4/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 408ms/step - loss: 2537.3665 - mae: 33.6906 - val_loss: 5969.8506 - val_mae: 49.0746 - learning_rate: 5.0000e-04\n",
            "Epoch 5/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 410ms/step - loss: 2160.4573 - mae: 30.9171 - val_loss: 6097.5972 - val_mae: 49.9342 - learning_rate: 5.0000e-04\n",
            "Epoch 6/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 434ms/step - loss: 1962.2699 - mae: 29.4848 - val_loss: 5902.1641 - val_mae: 48.6842 - learning_rate: 5.0000e-04\n",
            "Epoch 7/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 433ms/step - loss: 1670.0311 - mae: 27.0330 - val_loss: 5892.5464 - val_mae: 49.1388 - learning_rate: 5.0000e-04\n",
            "Epoch 8/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 410ms/step - loss: 1481.0872 - mae: 25.4821 - val_loss: 5768.6860 - val_mae: 49.2181 - learning_rate: 5.0000e-04\n",
            "Epoch 9/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 411ms/step - loss: 1296.7437 - mae: 23.7231 - val_loss: 6092.3501 - val_mae: 49.5352 - learning_rate: 2.5000e-04\n",
            "Epoch 10/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 434ms/step - loss: 1166.9331 - mae: 22.5642 - val_loss: 6306.7666 - val_mae: 50.4971 - learning_rate: 2.5000e-04\n",
            "Epoch 11/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 441ms/step - loss: 1072.5597 - mae: 21.5792 - val_loss: 6037.5356 - val_mae: 50.1011 - learning_rate: 2.5000e-04\n",
            "Epoch 12/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 435ms/step - loss: 1002.9912 - mae: 21.0670 - val_loss: 6314.2305 - val_mae: 51.1193 - learning_rate: 2.5000e-04\n",
            "Epoch 13/40\n",
            "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 435ms/step - loss: 992.6223 - mae: 20.7959 - val_loss: 5842.3711 - val_mae: 49.0194 - learning_rate: 2.5000e-04\n",
            " Enhanced-LSTM - RMSE: 74.75, MAE: 48.68, Time: 2536.8s\n",
            "\n",
            " MODEL PERFORMANCE COMPARISON:\n",
            "                    rmse        mae         time\n",
            "GRU-Attention  71.862637  46.401725  1505.983170\n",
            "CNN-LSTM       73.574297  48.228032   716.165731\n",
            "Enhanced-LSTM  74.753373  48.676166  2536.789336\n",
            "Transformer    86.490473  59.422206   606.348080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENSEMBLE METHODS"
      ],
      "metadata": {
        "id": "tmNVAIWXMSle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Simple Average Ensemble\n",
        "val_predictions = np.array(list(model_predictions.values()))\n",
        "average_ensemble_pred = np.mean(val_predictions, axis=0)\n",
        "ensemble_rmse = np.sqrt(mean_squared_error(y_val_seq, average_ensemble_pred))\n",
        "ensemble_mae = mean_absolute_error(y_val_seq, average_ensemble_pred)\n",
        "\n",
        "print(f\" Average Ensemble - RMSE: {ensemble_rmse:.2f}, MAE: {ensemble_mae:.2f}\")\n",
        "\n",
        "# 2. Weighted Average Ensemble (weight by performance)\n",
        "weights = 1.0 / np.array([perf['rmse'] for perf in model_performance.values()])\n",
        "weights /= weights.sum()\n",
        "weighted_ensemble_pred = np.average(val_predictions, axis=0, weights=weights)\n",
        "weighted_rmse = np.sqrt(mean_squared_error(y_val_seq, weighted_ensemble_pred))\n",
        "weighted_mae = mean_absolute_error(y_val_seq, weighted_ensemble_pred)\n",
        "\n",
        "print(f\" Weighted Ensemble - RMSE: {weighted_rmse:.2f}, MAE: {weighted_mae:.2f}\")\n",
        "\n",
        "# 3. Stacking Ensemble with Meta-Learner\n",
        "# Use model predictions as features for meta-learner\n",
        "stacking_features = np.column_stack(list(model_predictions.values()))\n",
        "\n",
        "# Train meta-learner (Gradient Boosting)\n",
        "meta_learner = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "meta_learner.fit(stacking_features, y_val_seq)\n",
        "\n",
        "# Predict with meta-learner\n",
        "stacking_pred = meta_learner.predict(stacking_features)\n",
        "stacking_rmse = np.sqrt(mean_squared_error(y_val_seq, stacking_pred))\n",
        "stacking_mae = mean_absolute_error(y_val_seq, stacking_pred)\n",
        "\n",
        "print(f\" Stacking Ensemble - RMSE: {stacking_rmse:.2f}, MAE: {stacking_mae:.2f}\")\n",
        "\n",
        "# Choose best ensemble method\n",
        "ensemble_results = {\n",
        "    'Average': ensemble_rmse,\n",
        "    'Weighted': weighted_rmse,\n",
        "    'Stacking': stacking_rmse\n",
        "}\n",
        "\n",
        "best_ensemble_method = min(ensemble_results, key=ensemble_results.get)\n",
        "print(f\"\\n Best Ensemble Method: {best_ensemble_method} (RMSE: {ensemble_results[best_ensemble_method]:.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C5Y1hWLMTlw",
        "outputId": "12a2dd60-9034-429f-b3c7-71fff7654b23"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Average Ensemble - RMSE: 70.99, MAE: 47.06\n",
            " Weighted Ensemble - RMSE: 70.75, MAE: 46.79\n",
            " Stacking Ensemble - RMSE: 58.31, MAE: 39.49\n",
            "\n",
            " Best Ensemble Method: Stacking (RMSE: 58.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINAL PREDICTION & SUBMISSION"
      ],
      "metadata": {
        "id": "k8gQqXZAMnec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_seq = []\n",
        "current_sequence = X_train_scaled[-TIME_STEPS:].copy()\n",
        "\n",
        "for i in range(len(X_test_scaled)):\n",
        "    current_sequence = np.roll(current_sequence, -1, axis=0)\n",
        "    current_sequence[-1] = X_test_scaled[i]\n",
        "    X_test_seq.append(current_sequence.copy())\n",
        "\n",
        "X_test_seq = np.array(X_test_seq)\n",
        "print(f\"✅ Test sequences shape: {X_test_seq.shape}\")\n",
        "\n",
        "# Generate predictions from all models\n",
        "test_predictions_all = {}\n",
        "for name, model in trained_models.items():\n",
        "    test_predictions_all[name] = model.predict(X_test_seq, verbose=0).flatten()\n",
        "    print(f\" {name} predictions generated\")\n",
        "\n",
        "# Create ensemble prediction\n",
        "if best_ensemble_method == 'Average':\n",
        "    ensemble_test_pred = np.mean(list(test_predictions_all.values()), axis=0)\n",
        "elif best_ensemble_method == 'Weighted':\n",
        "    ensemble_test_pred = np.average(list(test_predictions_all.values()), axis=0, weights=weights)\n",
        "else:  # Stacking\n",
        "    stacking_test_features = np.column_stack(list(test_predictions_all.values()))\n",
        "    ensemble_test_pred = meta_learner.predict(stacking_test_features)\n",
        "\n",
        "# Ensure correct length\n",
        "test_original = pd.read_csv('/content/drive/My Drive/air_quality/data/test.csv')\n",
        "if len(ensemble_test_pred) > len(test_original):\n",
        "    ensemble_test_pred = ensemble_test_pred[:len(test_original)]\n",
        "elif len(ensemble_test_pred) < len(test_original):\n",
        "    last_pred = ensemble_test_pred[-1] if len(ensemble_test_pred) > 0 else 0\n",
        "    pad_needed = len(test_original) - len(ensemble_test_pred)\n",
        "    ensemble_test_pred = np.append(ensemble_test_pred, [last_pred] * pad_needed)\n",
        "\n",
        "# Create submission with proper format\n",
        "def remove_leading_zeros(dt_str):\n",
        "    \"\"\"Remove leading zeros from datetime string\"\"\"\n",
        "    if ' ' in str(dt_str) and ':' in str(dt_str):\n",
        "        date_part, time_part = str(dt_str).split(' ')\n",
        "        time_parts = time_part.split(':')\n",
        "        if time_parts[0].startswith('0') and len(time_parts[0]) == 2:\n",
        "            time_parts[0] = time_parts[0][1]\n",
        "        return f\"{date_part} {':'.join(time_parts)}\"\n",
        "    return str(dt_str)\n",
        "\n",
        "formatted_dates = [remove_leading_zeros(dt) for dt in test_original['datetime']]\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'row ID': formatted_dates,\n",
        "    'pm2.5': np.clip(ensemble_test_pred, 0, None).astype(int)\n",
        "})\n",
        "\n",
        "# Save submission\n",
        "save_dir = '/content/drive/MyDrive/Kaggle_competition_ML/air_quality_forcasting'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "submission_file = os.path.join(save_dir, 'submission_ensemble.csv')\n",
        "submission.to_csv(submission_file, index=False)\n",
        "\n",
        "print(f\" Ensemble submission saved: {submission_file}\")\n",
        "\n",
        "# Verify submission\n",
        "print(\"\\n Submission verification:\")\n",
        "print(f\"Shape: {submission.shape}\")\n",
        "print(\"First 5 rows:\")\n",
        "for i in range(5):\n",
        "    print(f\"  {submission['row ID'].iloc[i]}, {submission['pm2.5'].iloc[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvMEPMwjMjct",
        "outputId": "a95c30e5-8811-4494-ebde-40538cc2545f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Test sequences shape: (13148, 48, 40)\n",
            "✅ CNN-LSTM predictions generated\n",
            "✅ Transformer predictions generated\n",
            "✅ GRU-Attention predictions generated\n",
            "✅ Enhanced-LSTM predictions generated\n",
            "✅ Ensemble submission saved: /content/drive/MyDrive/Kaggle_competition_ML/air_quality_forcasting/submission_ensemble.csv\n",
            "\n",
            "🔍 Submission verification:\n",
            "Shape: (13148, 2)\n",
            "First 5 rows:\n",
            "  2013-07-02 4:00:00, 30\n",
            "  2013-07-02 5:00:00, 30\n",
            "  2013-07-02 6:00:00, 30\n",
            "  2013-07-02 7:00:00, 30\n",
            "  2013-07-02 8:00:00, 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " SECTION 7: COMPREHENSIVE ANALYSIS"
      ],
      "metadata": {
        "id": "lrZQbz40M_p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" FINAL RESULTS SUMMARY:\")\n",
        "print(f\"Best Individual Model: {performance_df['rmse'].idxmin()} (RMSE: {performance_df['rmse'].min():.2f})\")\n",
        "print(f\"Best Ensemble Method: {best_ensemble_method} (RMSE: {ensemble_results[best_ensemble_method]:.2f})\")\n",
        "\n",
        "print(\"\\n MODEL PERFORMANCE RANKING:\")\n",
        "performance_df = performance_df.sort_values('rmse')\n",
        "print(performance_df)\n",
        "\n",
        "print(\"\\n ENSEMBLE PERFORMANCE:\")\n",
        "for method, rmse in ensemble_results.items():\n",
        "    improvement = ((performance_df['rmse'].iloc[0] - rmse) / performance_df['rmse'].iloc[0]) * 100\n",
        "    print(f\"  {method}: RMSE {rmse:.2f} ({improvement:+.1f}% vs best individual)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EET6FoEJM_Y1",
        "outputId": "a4b9e126-f054-4d7b-e017-36f3e9167409"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " FINAL RESULTS SUMMARY:\n",
            "Best Individual Model: GRU-Attention (RMSE: 71.86)\n",
            "Best Ensemble Method: Stacking (RMSE: 58.31)\n",
            "\n",
            " MODEL PERFORMANCE RANKING:\n",
            "                    rmse        mae         time\n",
            "GRU-Attention  71.862637  46.401725  1505.983170\n",
            "CNN-LSTM       73.574297  48.228032   716.165731\n",
            "Enhanced-LSTM  74.753373  48.676166  2536.789336\n",
            "Transformer    86.490473  59.422206   606.348080\n",
            "\n",
            " ENSEMBLE PERFORMANCE:\n",
            "  Average: RMSE 70.99 (+1.2% vs best individual)\n",
            "  Weighted: RMSE 70.75 (+1.5% vs best individual)\n",
            "  Stacking: RMSE 58.31 (+18.9% vs best individual)\n"
          ]
        }
      ]
    }
  ]
}